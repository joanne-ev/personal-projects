{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f39348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b658d44a",
   "metadata": {},
   "source": [
    "# Scikit-Learn's Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0db34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pl.read_csv(\"Housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6f0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = housing.select(pl.col(pl.String)).columns\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_array = encoder.fit_transform(housing[categorical_cols]).toarray().astype('int64')\n",
    "encoder_features = encoder.get_feature_names_out().tolist()\n",
    "\n",
    "housing_cat = pl.DataFrame(\n",
    "    encoded_array,\n",
    "    schema=encoder_features\n",
    ").with_row_index(name=\"index\")\n",
    "\n",
    "housing_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f93c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_int = housing.clone().select(pl.col(pl.Int64)).with_row_index(name=\"index\")\n",
    "housing_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = housing_int.join(\n",
    "    other=housing_cat,\n",
    "    on=\"index\", \n",
    "    how=\"left\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca095c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "x = housing_df[:, 2:].to_numpy()\n",
    "y = housing_df[:, 0].to_numpy().reshape(-1, 1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732fa05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisation\n",
    "x_scaler = StandardScaler()\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0626c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "# Cross validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=state)\n",
    "\n",
    "# Applying the linear regression model on each fold\n",
    "cv_results = cross_validate(\n",
    "    lin_reg, \n",
    "    x_train_scaled, y_train_scaled,\n",
    "    scoring=\"neg_mean_squared_error\", \n",
    "    cv=cv,\n",
    "    return_estimator=True\n",
    ")\n",
    "\n",
    "# Identify which fold produced the best score\n",
    "best_fold_idx = cv_results['test_score'].round(3).argmax()\n",
    "\n",
    "# Get the fitted model corresponding to the best fold\n",
    "best_estimator = cv_results['estimator'][best_fold_idx]\n",
    "\n",
    "# Extract the results\n",
    "coefficients_scaled = best_estimator.coef_[0]\n",
    "intercept_scaled = best_estimator.intercept_[0]\n",
    "mse = -(cv_results['test_score'][best_fold_idx])\n",
    "\n",
    "# Intercept\n",
    "intercept_reshaped = intercept_scaled.reshape(1, -1)\n",
    "intercept = y_scaler.inverse_transform(intercept_reshaped)[0, 0].round(2)\n",
    "\n",
    "# Coefficients\n",
    "coefficients_reshaped = coefficients_scaled.reshape(1, -1)\n",
    "coefficients = x_scaler.inverse_transform(coefficients_reshaped)[0].round(3)\n",
    "\n",
    "## Final Results\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "print(\"### Best Fold Results ###\")\n",
    "print(f\"Score (MSE): {mse.round(3)}\")\n",
    "print(f\"Intercept: {intercept.round(3):,}\")\n",
    "print(f\"Coefficients: {coefficients.round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69519ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = best_estimator.predict(x_test_scaled)\n",
    "\n",
    "final_mse = mean_squared_error(y_test_scaled, y_pred_test)\n",
    "final_r2_score = best_estimator.score(x_test_scaled, y_test_scaled)\n",
    "\n",
    "print(f\"Final test MSE: {final_mse:.4f}\")\n",
    "print(f\"Final test R-squared score: {final_r2_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef8245",
   "metadata": {},
   "source": [
    "# PyTorch's Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafc836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "device = 'mps' if torch.mps.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad3447",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fcd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_df[:, 2:].to_numpy()\n",
    "Y = housing_df[:, 1].to_numpy().reshape(-1, 1)\n",
    "\n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X, Y, test_size=0.25, random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = X_val_test.shape[0]\n",
    "test_indices = np.arange(nrows) \n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "test_val_split = round(nrows * 0.50)\n",
    "val_idx = test_indices[:test_val_split] \n",
    "test_idx = test_indices[test_val_split:] \n",
    "\n",
    "X_test, Y_test = X_val_test[test_idx], Y_val_test[test_idx]\n",
    "X_val, Y_val = X_val_test[val_idx], Y_val_test[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ea964",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "\n",
    "X_val_scaled = X_scaler.transform(X_val)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "Y_scaler = StandardScaler()\n",
    "Y_train_scaled = Y_scaler.fit_transform(Y_train)\n",
    "Y_train_tensor = torch.tensor(Y_train_scaled, dtype=torch.float32)\n",
    "\n",
    "Y_val_scaled = Y_scaler.transform(Y_val)\n",
    "Y_val_tensor = torch.tensor(Y_val_scaled, dtype=torch.float32)\n",
    "\n",
    "Y_test_scaled = Y_scaler.transform(Y_test)\n",
    "Y_test_tensor = torch.tensor(Y_test_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b890f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "validation_data = TensorDataset(X_val_tensor, Y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing model architecture\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()  # Inherit constructor from parent class (nn.Module) to initialise the parent class\n",
    "\n",
    "        # Build sequential layer\n",
    "        self.sequential_layer = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),  # 50% of the input features will be randomly set to zero\n",
    "            nn.Linear(input_size, 10),  # Linear layer 1\n",
    "            nn.ReLU(),  # Rectified linear unit (activation function)\n",
    "            nn.Linear(10, output_size),  # Linear layer 2\n",
    "        )\n",
    "\n",
    "    # The forward pass simply calls the sequential container\n",
    "    def forward(self, x):\n",
    "        return self.sequential_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08aafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    # Initialise trainer with the core components\n",
    "    def __init__(self, model, loss_criterion, optimiser):\n",
    "        self.model = model\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.optimiser = optimiser\n",
    "\n",
    "    # Behaviours within a singular training step\n",
    "    def training_step(self, x, y):\n",
    "        # Set model to training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Forward pass (i.e., create prediction based on features)\n",
    "        y_hat = self.model(x)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.loss_criterion(y_hat, y)\n",
    "\n",
    "        # Backwards pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimisation\n",
    "        self.optimiser.step()\n",
    "\n",
    "        # Reset gradient to zero\n",
    "        self.optimiser.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    # Trains the model over multiple epochs using the provided loaded data\n",
    "    def training_process(self, training_batch, epochs, print_loss=False):\n",
    "        total_train_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_batch_loss = []\n",
    "\n",
    "            for x_train_batch, y_train_batch in training_batch:\n",
    "                x_train_batch = x_train_batch.to(device)\n",
    "                y_train_batch = y_train_batch.to(device)\n",
    "\n",
    "                train_loss = self.training_step(x_train_batch, y_train_batch)\n",
    "                train_batch_loss.append(train_loss)\n",
    "\n",
    "            total_train_loss.append(torch.tensor(train_batch_loss).mean().item())\n",
    "\n",
    "            if print_loss == True:\n",
    "                if epoch == 0:\n",
    "                    print(f\"Epoch {epoch}/{epochs} | Train Loss: {total_train_loss[-1]:.3f}\")\n",
    "                if (epoch + 1) % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch + 1}/{epochs} | Train Loss: {total_train_loss[-1]:.3f}\"\n",
    "                    )\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return total_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_process(model, validation_batch, loss_criterion, epochs, print_loss=False):\n",
    "    total_val_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        with torch.no_grad():\n",
    "            val_batch_loss = []\n",
    "            \n",
    "            for x_val_batch, y_val_batch in validation_batch:\n",
    "                # Move validation data to the device\n",
    "                x_val_batch = x_val_batch.to(device)\n",
    "                y_val_batch = y_val_batch.to(device)\n",
    "                \n",
    "                model.eval()\n",
    "                \n",
    "                y_hat = model(x_val_batch)\n",
    "                val_loss = loss_criterion(y_hat, y_val_batch).item()\n",
    "                val_batch_loss.append(val_loss)\n",
    "                \n",
    "            total_val_loss.append(torch.tensor(val_batch_loss).mean().item())\n",
    "        \n",
    "        if print_loss == True:\n",
    "            if epoch == 0:\n",
    "                print(\n",
    "                        f\"Epoch {epoch} / {epochs} | Validation loss: {total_val_loss[-1]:.3f}\"\n",
    "                    )\n",
    "                if (epoch + 1) % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch + 1}/{epochs} | Validation loss: {total_val_loss[-1]:.3f}\"\n",
    "                    )\n",
    "    \n",
    "    return total_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43afdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter tuning ###\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'lr': [1e-4, 1e-3],\n",
    "    'weight_decay': [1e-5, 1e-4],\n",
    "    'batch_size': [4, 8],\n",
    "    'epochs' : [2000, 4000]\n",
    "}\n",
    "\n",
    "# Perform grid search to find the best parameters\n",
    "best_params = None\n",
    "best_loss = float('inf')  # Initialise best_loss as infinity\n",
    "\n",
    "# Go through a grid with all possible combinations of the specified hyperparameters\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "\n",
    "    # Get data loaders for current batch size\n",
    "    train_batch_tuning = DataLoader(dataset=training_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "    # Initialise model, loss function, and optimiser with current parameters\n",
    "    model = LinearModel(input_size=X.shape[1], output_size=1)\n",
    "    model.to(device) # Move the model to the device (if on cuda)\n",
    "\n",
    "    tune_mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "    optimiser = optim.SGD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = ModelTrainer(model=model, loss_criterion=tune_mse_loss, optimiser=optimiser\n",
    "    ).training_process(training_batch=train_batch_tuning, epochs=params['epochs'], print_loss = False)\n",
    "\n",
    "    train_loss_mean = np.mean(train_loss).item()\n",
    "\n",
    "    # Update best parameters if current validation loss is lower\n",
    "    if train_loss_mean < best_loss:\n",
    "        best_loss = train_loss_mean\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f3438",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameters: {best_params} with the lowest loss of {best_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ec3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch_tuning = DataLoader(dataset=validation_data, batch_size=params['batch_size'], shuffle=True)\n",
    "\n",
    "# Validate the model\n",
    "val_loss = validation_process(model=model, validation_batch=val_batch_tuning, loss_criterion=mse_loss, epochs=n_epochs)\n",
    "\n",
    "val_loss_mean = np.mean(val_loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5338706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_process(model, loss_criterion, test_batches):\n",
    "    with torch.no_grad():\n",
    "        total_test_loss = []\n",
    "\n",
    "        for x_test_batch, y_test_batch in test_batches:\n",
    "            # Move validation data to the device\n",
    "            x_test_batch = x_test_batch.to(device)\n",
    "            y_test_batch = y_test_batch.to(device)\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            y_hat = model(x_test_batch)\n",
    "            test_loss = loss_criterion(y_hat, y_test_batch).item()\n",
    "            total_test_loss.append(test_loss)\n",
    "        \n",
    "    return total_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66692cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(input_size=X.shape[1], output_size=1)\n",
    "test_mse_loss = nn.MSELoss(reduction=\"mean\")\n",
    "testing_data = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "test_batches = DataLoader(dataset=testing_data, batch_size=best_params['batch_size'], shuffle=True)\n",
    "test_optimiser = optim.SGD(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "\n",
    "total_test_loss = testing_process(model=model, loss_criterion=mse_loss, test_batches=test_batches)\n",
    "print(f\"Average MSE: {np.mean(total_test_loss):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705d1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(np.arange(len(X_test_tensor)), total_test_loss)\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"MSE Loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
